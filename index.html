<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="WHOOPS!">
  <meta property="og:title" content="WHOOPS! Benchmark"/>
  <meta property="og:description" content="NeuroPrompts: An Adaptive Framework to Optimize Prompts for Text-to-Image Generation"/>
  <meta property="og:url" content="https://whoops-benchmark.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="WHOOPS! Benchmark">
  <meta name="twitter:description" content="NeuroPrompts: An Adaptive Framework to Optimize Prompts for Text-to-Image Generation">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/nitzangu_Albert_Einstein_holds_an_iphone_realistic_style_ba15ea7b-b829-478b-adef-b474ccf01a2a.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision-and-Language Benchmark, Synthetic and Compositional Images">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>WHOOPS! Benchmark </title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.3.0/css/all.min.css">-->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">NeuroPrompts: An Adaptive Framework to Optimize Prompts for Text-to-Image Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=-8JzBBEAAAAJ&hl=en" target="_blank">Shachar Rosenman</a>,&nbsp;</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=Qbu4oKwAAAAJ&hl=en" target="_blank">Vasudev Lal</a>,&nbsp;</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=EKh822gAAAAJ&hl=en" target="_blank">Phillip Howard</a>,&nbsp;</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Intel Labs,&nbsp;&nbsp;</span>
                  </div>

<!--                  &lt;!&ndash; Github link &ndash;&gt;-->
<!--                  <span class="link-block">-->
<!--                    <a href="https://github.com/YOUR REPO HERE" target="_blank"-->
<!--                    class="external-link button is-normal is-rounded is-dark">-->
<!--                    <span class="icon">-->
<!--                      <i class="fab fa-github"></i>-->
<!--                    </span>-->
<!--                    <span>Code</span>-->
<!--                  </a>-->
<!--                </span>-->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2303.07274" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                <!-- Demo Link -->
                <span class="link-block">
                  <a href="http://3.131.193.145:7860/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:20px">&#x1F917;</p>
                  </span>
                  <span>Demo</span>
                </a>
              </span>

              <span class="link-block">
                  <a href="https://github.com/IntelLabs/multimodal_cognitive_ai/tree/main/Demos/NeuroPrompts" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <p style="font-size:20px">&#x1F917;</p>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <span class="link-block">
                  <a href="https://youtu.be/Cmca_RWYn2g" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-code"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>   

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
	
<section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-centered">
          NeuroPrompts is an interface to Stable Diffusion which automatically optimizes a user’s prompt for improved image aesthetics while maintaining stylistic control according to the user’s preferences.
        </h4>
      </div>
    </div>
</section>

<section class="section"  style="background-color:#efeff081">
    <div class="container is-max-desktop" id="gradio">
      <gradio-app src="http://3.131.193.145:7860/"></gradio-app>
    </div>
</section>

<section class="section"  style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Despite impressive recent advances in text-to-image diffusion models, obtaining high-quality images often requires prompt engineering by humans who have developed expertise in using them.
		    In this work, we present NeuroPrompts, an adaptive framework that automatically enhances a user’s prompt to improve the quality of generations produced by text-to-image mod- els.
		    Our framework utilizes constrained text de- coding with a pre-trained language model that has been adapted to generate prompts similar to those produced by human prompt engineers,
		    which enables higher-quality text-to-image generations from resulting prompts and provides user control over stylistic features via constraint set specification.
		    We demonstrate the utility of our framework by creating an interactive application for prompt enhancement and image generation using Stable Diffusion.
		    Additionally, we conduct experiments utilizing a large dataset of human-engineered prompts for text-to-image generation and show that our approach automatically produces
		    enhanced prompts that result in superior image quality.
           </p>
  
          </div>
        </div>
      </div>
        
    </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="publication-flipped">
        <h2 class="title is-3">What makes these images weird?</h2>
        <div class="content has-text-justified">
<!--            CLICK TO GET AN ANSWER!-->
        <div class="flip-box-container">
          <div class="flip-box">
            <div class="flip-box-inner">
              <div class="flip-box-front">
                <img src="static/images/nitzangu_Albert_Einstein_holds_an_iphone_realistic_style_ba15ea7b-b829-478b-adef-b474ccf01a2a.png" alt="Paris" style="width:300px;height:300px">
              </div>
              <div class="flip-box-back">
                <h2><br><br>Albert Einstein is holding a smartphone</h2>
                <p class="has-text-weight-bold">Albert Einstein died in 1955, decades before the first smartphone was invented (1994)</p>
              </div>
            </div>
          </div>

          <div class="flip-box">
            <div class="flip-box-inner">
              <div class="flip-box-front">
                <img src="static/images/712a29029099a1a4a85139666a6d6062407eb4511fa0d48789001413678ac91a.png" alt="Paris" style="width:300px;height:300px">
              </div>
              <div class="flip-box-back">
                <h2><br><br>A lit candle is sitting inside a tightly sealed glass jar</h2>
                <p class="has-text-weight-bold">A candle needs a constant supply of oxygen to burn, which does not exist in a sealed bottle, so it is unlikely to see a burning candle inside a sealed bottle</p>
              </div>
            </div>
          </div>

         <div class="flip-box">
            <div class="flip-box-inner">
              <div class="flip-box-front">
                <img src="static/images/1dfcec6aaf52317654922201bfdc1fb40cd0ab130144424f54ac26c3f5f3a082.png" alt="Paris" style="width:300px;height:300px">
              </div>
              <div class="flip-box-back">
                <h2><br><br>A wolf howls at the sun</h2>
                <p class="has-text-weight-bold">Wolves are known to howl on a full moon, so it may not be customary to do it in the middle of the day</p>
              </div>
            </div>
          </div>

        </div>
<!--        </p>-->
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

       <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Abstract
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left has-background-info-light pr-4 pl-4 pt-3 pb-3">
         <p>
         Weird, unusual, and uncanny images pique the curiosity of observers because they challenge commonsense.
           <br>For example, an image released during the 2022 world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo playing chess, which playfully violates our expectation that their competition should occur on the football field.
           Humans can easily recognize and interpret these unconventional images, but can AI models do the same? We introduce WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney.
           <br>We consider several tasks posed over the dataset. In addition to image captioning, cross-modal matching, and visual question answering, we introduce a difficult explanation generation task, where models must identify and explain why a given image is unusual.
           <br>Our results show that state-of-the-art models such as GPT3 and BLIP2 still lag behind human performance on WHOOPS!. We hope our dataset will inspire the development of AI models with stronger visual commonsense reasoning abilities.
         </p>
         </h3>
      </div>

      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Collecting Weird Images
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
         WHOOPS! is a dataset of 500 synthetic images and 10,874 annotations designed to challenge AI models' ability to reason about commonsense and compositionality. To construct WHOOPS!, we collaborate with designers who use text-to-image models such as Midjourney and DALL-E to generate images that would be challenging (or even impossible) to collect otherwise.
         WHOOPS! contains commonsense-defying image from a wide range of reasons, deviations from expected social norms and everyday knowledge.
           <br><br>
           <img src="static/images/clusters.png"  alt="MY ALT TEXT"/>
         </p>
         </h3>
      </div>

      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Using Weird Images to Create V&L Tasks
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
         <p>
         The WHOOPS! benchmark includes four tasks:
            <ol class="pr-2 pl-6">
              <li>A novel task of explanation-of-violation: generating a detailed explanation for what makes the image weird</li>
              <li>Generating a literal caption</li>
              <li>Distinguishing between detailed and underspecified captions</li>
             <li>Answering questions that test compositional understanding</li>
            </ol>
             <br><br>
           <img src="static/images/benchmarking.png"/>
         </p>
         </h3>
      </div>

      <div class="item">
         <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Test Results for Explanation-of-violation
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
           <p>Models significantly lag behind human performance. For example, on identification, the best end-to-end fine-tuned BLIP2 FlanT5-XXL model achieves at best 73%. For explanation, even the oracle model (which is given access to a ground-truth, human-authored description of the image) only achieves a performance of 68%, falling substantially short of human performance (95%). We also added auto-eval results that are correlated with the human-eval. These results indicate that our dataset provides a challenging benchmark for the development of next-generation vision-and-language models.</p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/Tb1.png"  style="width: 80%; height: 80%"/>
         </p>
         </h3>
      </div>

      <div class="item">
        <h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Test Results for Image Captioning, Cross-Modal Matching and Visual Question Answering
           </h2>
         <h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
           <p>The zero-shot results highlight the strengths and weaknesses of each model. Zero-shot BLIP2 demonstrates a substantial improvement over the other models. But even the supervised models have significant room for improvement, especially in VQA (maximum BEM score is 57%) and image captioning
           </p>
         <p style="text-align:center;">
           <br><br>
           <img src="static/images/Tb2.png"  style="width: 80%; height: 80%"/>
         </p>
         </h3>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.16.2/gradio.js"></script>

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
          <div class="publication-video">
            <gradio-app src="https://nlphuji-whoops-explorer.hf.space"></gradio-app>
          </div>
        </div>
      </div>
    </div>
</section>
<!-- End youtube video -->

<!-- Youtube video -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h3 class="subtitle is-size-4-tablet has-background-info-light has-text-left pr-4 pl-4 pt-3 pb-3">
      We collect <i>normal</i> (synthetic, not weird) and <i>natural</i> (non-synthetic, not weird) images to investigate the main challenge in WHOOPS!. BLIP2 model performs well on <i>non-weird</i> cases but struggles on weird ones, indicating that weirdness is the primary challenge, not synthesis.
      </h3>
      <div class="columns is-centered has-text-centered">
          <div class="publication-video">
            <iframe src="https://nlphuji-whoops-explorer-analysis.hf.space" frameborder="0" width="850" height="450"></iframe>
          </div>
        </div>
      </div>
    </div>
</section>


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper</h2>
      <iframe  src="static/pdfs/WHOOPS_paper.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->

<script
	type="module"
	src="https://gradio.s3-us-west-2.amazonaws.com/3.36.1/gradio.js"></script>
<!-- Youtube video -->
<section class="hero is-small">
    <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
          <div class="publication-video">
            <gradio-app src="https://nlphuji-whoops-leaderboard.hf.space"></gradio-app>
          </div>
        </div>
      </div>
    </div>
</section>

<!--BibTex citation -->
<section class="hero is-small is-light">
  <div class="hero-body">
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{https://doi.org/10.48550/arxiv.2303.07274,
        doi = {10.48550/ARXIV.2303.07274},

        url = {https://arxiv.org/abs/2303.07274},

        author = {Bitton-Guetta, Nitzan and Bitton, Yonatan and Hessel, Jack and Schmidt, Ludwig and Elovici, Yuval and Stanovsky, Gabriel and Schwartz, Roy},

        keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},

        title = {Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images},

        publisher = {arXiv},

        year = {2023},

        copyright = {arXiv.org perpetual, non-exclusive license}
      }</code></pre>
    </div>
</section>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p style="color:gray;font-size:9.9px;">
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>, is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            <a href="https://www.flaticon.com/free-icons/magic" title="magic icons">Magic icons created by Freepik - Flaticon</a>
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
  </html>
